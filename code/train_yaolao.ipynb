{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e43429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69205d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 配置 ==========\n",
    "MODEL_NAME = \"/model/HuggingFace/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"  # 或者你在 HF 上的 chatglm3-6b 的 repo 名\n",
    "YAOLAO_JSON = \"/root/yaolao/train_data_filtered.json\"\n",
    "OUTPUT_DIR = \"lora_DeepSeek-R1-Distill-Qwen-14B_yaolao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练超参（按需调整）\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "PER_DEVICE_BATCH_SIZE = 2  # 取决于显存\n",
    "GRAD_ACCUM_STEPS = 8\n",
    "WEIGHT_DECAY = 0.0\n",
    "LOGGING_STEPS = 50\n",
    "SAVE_STEPS = 500\n",
    "MAX_LENGTH = 1024  # 根据模型和显存调整\n",
    "# LoRA 配置（按需调整）\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"v_proj\"]  # 常见于 transformer 层，chatglm 具体实现可能不同；如果无效可换成 [\"query_key_value\"] 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 辅助函数：把 conversations 拼成训练样本 ==========\n",
    "def build_examples_from_conversations(conversations: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    将你给出的 conversation list 转成多条训练样本：\n",
    "    每一条样本的 input 是 system + (user + assistant)* 到 assistant 之前的上下文，\n",
    "    label 仅为 assistant 的回复（user/system 部分 label = -100）。\n",
    "    假定 conversations 内 role 顺序是 system/user/assistant/user/assistant...\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    # 把整个 conversation 对话按轮次拼接。我们把每一次 assistant 回复作为一个训练目标\n",
    "    system_content = \"\"\n",
    "    idx = 0\n",
    "    # 先找 system，如果存在则记录\n",
    "    if len(conversations) and conversations[0].get(\"role\") == \"system\":\n",
    "        system_content = conversations[0].get(\"content\", \"\")\n",
    "        idx = 1\n",
    "\n",
    "    # 从 idx 开始遍历，构造上下文\n",
    "    history: List[Dict] = []\n",
    "    # 收集对话为一系列 (user, assistant)\n",
    "    while idx < len(conversations):\n",
    "        role = conversations[idx].get(\"role\")\n",
    "        if role == \"user\":\n",
    "            user_content = conversations[idx].get(\"content\", \"\")\n",
    "            # 如果后面有 assistant，作为一轮\n",
    "            if idx + 1 < len(conversations) and conversations[idx + 1].get(\"role\") == \"assistant\":\n",
    "                assistant_content = conversations[idx + 1].get(\"content\", \"\")\n",
    "                # 构造样本：上下文为 system + 历史轮 + 当前 user，label 为 assistant\n",
    "                # 保存当前历史（包括之前的 user/assistant）\n",
    "                rounds = history.copy()\n",
    "                rounds.append({\"user\": user_content, \"assistant\": None})\n",
    "                # build prompt text for input (we will append assistant target in labels handling)\n",
    "                prompt = \"\"\n",
    "                if system_content:\n",
    "                    prompt += f\"<|system|>:\\n{system_content}\\n\"\n",
    "                # 每轮用一个简单模板拼接（你可以根据 chatglm 期待的特殊标记修改）\n",
    "                for r in rounds[:-1]:  # 历史完整轮（都有 assistant）\n",
    "                    prompt += f\"<|user|>:\\n{r['user']}\\n<|assistant|>:\\n{r['assistant']}\\n\"\n",
    "                # 最后一轮只加 user（assistant 部分用作 label）\n",
    "                prompt += f\"<|user|>:\\n{user_content}\\n<|assistant|>:\\n\"\n",
    "\n",
    "                examples.append({\n",
    "                    \"input_text\": prompt,\n",
    "                    \"target_text\": assistant_content\n",
    "                })\n",
    "\n",
    "                # 更新历史：把这轮完整加入 history\n",
    "                history.append({\"user\": user_content, \"assistant\": assistant_content})\n",
    "                idx += 2\n",
    "            else:\n",
    "                # 不成对（没有 assistant），只保存 user 进历史\n",
    "                history.append({\"user\": user_content, \"assistant\": None})\n",
    "                idx += 1\n",
    "        else:\n",
    "            # 如果遇到 assistant 单独项或其他，直接跳过以防格式不规范\n",
    "            idx += 1\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 主流程 ==========\n",
    "def main():\n",
    "    # 1. 读取 json\n",
    "    with open(YAOLAO_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    # raw 可能是 {\"conversations\":[{...},{...}], ...} 或者是一个列表，每条为一段对话\n",
    "    # 适配常见两种情况\n",
    "    examples_all = []\n",
    "    if isinstance(raw, dict) and \"conversations\" in raw and isinstance(raw[\"conversations\"], list):\n",
    "        # 单条 conversation（整体）\n",
    "        examples_all += build_examples_from_conversations(raw[\"conversations\"])\n",
    "    elif isinstance(raw, list):\n",
    "        # 每个元素可能是一段 conversations\n",
    "        for item in raw:\n",
    "            if \"conversations\" in item:\n",
    "                examples_all += build_examples_from_conversations(item[\"conversations\"])\n",
    "    else:\n",
    "        raise ValueError(\"无法识别 yaolao.json 的结构，请确认根结构是 dict 且包含 conversations，或是 list。\")\n",
    "\n",
    "    print(f\"构造了 {len(examples_all)} 个训练样本示例（每个样本对应一条 assistant 回复）\")\n",
    "\n",
    "    # 2. 加载 tokenizer + 模型（可按需加 trust_remote_code）\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    # chatglm 系列有些 tokenizer 需要设置 truncation_side etc.\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    # 通过 bitsandbytes 低精度加载模型（如果没有 bitsandbytes 可去掉 load_in_8bit 参数）\n",
    "    # 准备 model（k-bit friendly）\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=True,   # 如果你不使用 bitsandbytes 删除或改为 False\n",
    "        llm_int8_enable_fp32_cpu_offload=True\n",
    "    )\n",
    "\n",
    "    # 使模型为 k-bit/8bit 微调做好准备\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    # 3. 应用 LoRA（PEFT）\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=TARGET_MODULES,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    print(\"已加载模型并应用 LoRA。可训练参数量（参数名/形状）：\")\n",
    "    # 打印可训练参数数量\n",
    "    trainable_params = 0\n",
    "    total_params = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        num = p.numel()\n",
    "        total_params += num\n",
    "        if p.requires_grad:\n",
    "            trainable_params += num\n",
    "    print(f\"Trainable params: {trainable_params} / Total params: {total_params}\")\n",
    "\n",
    "    # 4. 构造 Dataset（tokenize 并且 labels 仅包含 assistant）\n",
    "    def tokenize_and_build_labels(example):\n",
    "        # 把 input_text 和 target_text 合并： input_ids = tokenize(input_text + target_text)\n",
    "        # labels = [-100]*len(input_ids(input_text)) + token_ids(target_text)\n",
    "        # 这样 trainer 只会对 assistant 部分计算 loss\n",
    "        input_text = example[\"input_text\"]\n",
    "        target_text = example[\"target_text\"]\n",
    "\n",
    "        # 直接拼接\n",
    "        full_text = input_text + target_text\n",
    "        # tokenize\n",
    "        input_enc = tokenizer(\n",
    "            full_text,\n",
    "            max_length=MAX_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=False,  # 由 data collator 负责 padding\n",
    "        )\n",
    "        input_ids = input_enc[\"input_ids\"]\n",
    "\n",
    "        # token length of the input_text part\n",
    "        input_text_ids = tokenizer(input_text, add_special_tokens=False)[\"input_ids\"]\n",
    "        input_len = len(input_text_ids)\n",
    "\n",
    "        labels = [-100] * input_len + input_ids[input_len:]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        if len(input_ids) > MAX_LENGTH:\n",
    "            input_ids = input_ids[:MAX_LENGTH]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": [1] * len(input_ids),\n",
    "        }\n",
    "\n",
    "    ds = Dataset.from_list(examples_all)\n",
    "    ds_tokenized = ds.map(lambda x: tokenize_and_build_labels(x), remove_columns=ds.column_names, num_proc=1)\n",
    "\n",
    "    # 5. Data collator（使用 transformers 的 DataCollatorForSeq2Seq 或 自定义）\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=True)\n",
    "\n",
    "    # 6. TrainingArguments & Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "        warmup_ratio=0.03,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        fp16=True,\n",
    "        logging_steps=LOGGING_STEPS,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=3,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",  # 如果希望用 wandb 或 tensorboard，请配置\n",
    "        optim=\"paged_adamw_8bit\",  # 结合 bitsandbytes 推荐的优化器\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=ds_tokenized,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # 7. 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 8. 保存 LoRA 权重（只保存 adapter）\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"训练完成并将 LoRA 权重保存到 {OUTPUT_DIR}\")\n",
    "\n",
    "    # 9. 如果需要，将最终模型推到 hub（可选）\n",
    "    # from huggingface_hub import login\n",
    "    # login(token=\"你的HfToken\")\n",
    "    # model.push_to_hub(\"your-username/lora-chatglm3-yaolao\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaolao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
