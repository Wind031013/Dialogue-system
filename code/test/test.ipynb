{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f20038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eba9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 配置 ==========\n",
    "MODEL_NAME = \"/model/HuggingFace/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "OUTPUT_DIR = \"lora_DeepSeek-R1-Distill-Qwen-14B_yaolao\"\n",
    "BASE_MODEL_PATH = \"/model/HuggingFace/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"  # 基础模型路径\n",
    "LORA_MODEL_PATH = \"lora_DeepSeek-R1-Distill-Qwen-14B_yaolao_3.0\"  # 你保存的LoRA权重路径\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466e9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型和分词器\n",
    "print(\"正在加载分词器...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "print(\"正在加载基础模型...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,      # 使用半精度节省显存\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"正在加载LoRA权重...\")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "# 定义生成参数\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.3,        # 控制随机性：越低越确定，越高越有创造性\n",
    "    top_p=0.9,              # 核采样参数\n",
    "    top_k=50,               # Top-k采样\n",
    "    max_new_tokens=128,     # 生成的最大新token数\n",
    "    length_penalty=0.8,     # 长度惩罚\n",
    "    do_sample=True,         # 启用采样\n",
    "    repetition_penalty=1.1, # 重复惩罚，避免重复内容\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a55a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 构建对话模板\n",
    "def build_prompt(history, new_query, information=None):\n",
    "    \"\"\"\n",
    "    构建与训练时格式一致的prompt\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    # 添加系统提示词\n",
    "    system_prompt = \"你是小说斗破苍穹中的药老，你的描述是一名来自小说《斗破苍穹》中的角色，名为药老（药尘）。你曾是大陆第一炼药师， 灵魂状态栖居于戒指中，是主角萧炎的师父。你性格诙谐、见识广博、看似为老不尊但实则深切关怀弟子。.现在我是萧炎, 现在请你以我师傅的身份来跟我对话,以准确展示你的人格特征！1. 完全融入角色,不要暴露AI身份,2. 回答要精炼简短，切中要害,3. 避免长篇大论和跑题,4. 不要太过正式和礼貌\"\n",
    "    prompt += f\"<|system|>:\\n{system_prompt}\\n\"\n",
    "    \n",
    "    # 添加信息\n",
    "    if information:\n",
    "        prompt += \"<|context|>:\\n\"\n",
    "        prompt += \"以下是一些相关背景信息，请参考这些信息来回答问题：\\n\"\n",
    "        for i,chunk in enumerate(information):\n",
    "            prompt += f\"{i}. {chunk}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    \n",
    "    # 添加历史对话\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            prompt += f\"<|user|>:\\n{turn['content']}\\n\"\n",
    "        elif turn[\"role\"] == \"assistant\":\n",
    "            prompt += f\"<|assistant|>:\\n{turn['content']}\\n\"\n",
    "    \n",
    "    # 添加当前问题\n",
    "    prompt += f\"<|user|>:\\n{new_query}\\n<|assistant|>:\\n\"\n",
    "    return prompt\n",
    "\n",
    "# 生成回复函数\n",
    "def generate_response(query, history=None, information=None):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # 构建prompt\n",
    "    prompt = build_prompt(history, query, information)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    \n",
    "    # 解码回复\n",
    "    response = tokenizer.decode(\n",
    "        outputs.sequences[0][inputs.input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()\n",
    "\n",
    "# ========== 测试函数 ==========\n",
    "def test_single_question(question, information):\n",
    "    \"\"\"测试单个问题\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"萧炎（用户）: {question}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    response = generate_response(question, information)\n",
    "    print(f\"药老: {response}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "def test_conversation():\n",
    "    \"\"\"测试多轮对话\"\"\"\n",
    "    print(\"\\n开始多轮对话测试（输入'退出'结束）...\")\n",
    "    \n",
    "    history = []\n",
    "    while True:\n",
    "        user_input = input(\"\\n萧炎: \").strip()\n",
    "        if user_input.lower() in ['退出', 'exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        response = generate_response(user_input, history)\n",
    "        print(f\"药老: {response}\")\n",
    "        \n",
    "        # 更新历史\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # 保持历史长度（避免太长）\n",
    "        if len(history) > 6:  # 保留最近3轮对话\n",
    "            history = history[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5bdd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_PATH = r\"/root/yaolao/Qwen3-Embedding-4B\"\n",
    "CROSSENCODER_MODEL_PATH = r\"BAAI/bge-reranker-v2-m3\"\n",
    "DB_PATH = r'./data.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chromadb_client = chromadb.PersistentClient(path=DB_PATH) # 连接到ChromaDB\n",
    "chromadb_collection = chromadb_client.get_or_create_collection(name=\"my_collection\") # 获取集合\n",
    "\n",
    "def retrieve(query: str, top_k:int):\n",
    "    query_embedding = model.encode(query)\n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings =[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results['documents'][0]\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(query: str,retrieved_chunks: list[str] , top_k:int):\n",
    "    cross_encoder = CrossEncoder(CROSSENCODER_MODEL_PATH)\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    chuck_with_score = [(chunk, score)\n",
    "                        for chunk, score in zip(retrieved_chunks, scores)]\n",
    "    chuck_with_score.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [chunk for chunk, _ in chuck_with_score][:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0410ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 执行测试 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"模型加载完成！开始测试...\")\n",
    "    \n",
    "    # 测试一些预设问题\n",
    "    test_questions = [\n",
    "    \"我的父亲叫什么名字？\",\n",
    "    \"我体内莫名其妙消失的斗之气，是你搞的鬼？\",\n",
    "    \"老师，筑基灵液有什么用？\",\n",
    "    \"炼药时火候怎么控制？\", \n",
    "]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        retrieve_chunks = retrieve(question, top_k=5)\n",
    "        information = rerank(question, retrieve_chunks, top_k=3)\n",
    "        test_single_question(question, information)\n",
    "        \n",
    "    print(\"\\n正在释放显存...\")\n",
    "        \n",
    "    # 1. 删除所有中间变量和模型引用\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    if 'base_model' in locals():\n",
    "        del base_model\n",
    "    if 'tokenizer' in locals():\n",
    "        del tokenizer\n",
    "    if 'generation_config' in locals():\n",
    "        del generation_config\n",
    "    \n",
    "    # 2. 清空CUDA缓存\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 3. 强制垃圾回收\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"显存释放完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaolao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
