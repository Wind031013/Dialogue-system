{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e43429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import gc\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69205d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置路径\n",
    "MODEL_NAME = \"/model/HuggingFace/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "# YAOLAO_JSON = \"/root/yaolao/train_data_filtered.json\"\n",
    "YAOLAO_JSON = r\"E:\\论文\\train_data\\train_data_prompted2.json\"\n",
    "OUTPUT_DIR = \"lora_DeepSeek-R1-Distill-Qwen-14B_yaolao\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练超参\n",
    "NUM_EPOCHS = 3                              # 训练轮数\n",
    "LEARNING_RATE = 2e-4                        # 学习率        \n",
    "PER_DEVICE_BATCH_SIZE = 2                   # 每个设备的批大小（取决于显存）\n",
    "GRAD_ACCUM_STEPS = 8                        # 梯度累积步数   \n",
    "WEIGHT_DECAY = 0.0                          # 权重衰减\n",
    "LOGGING_STEPS = 50                          # 日志记录步数\n",
    "SAVE_STEPS = 500                            # 模型保存步数\n",
    "MAX_LENGTH = 1024                           # 最大输入长度\n",
    "\n",
    "# LoRA 配置\n",
    "LORA_R = 16                                 # 秩（rank）\n",
    "LORA_ALPHA = 32                             # alpha         （缩放因子）\n",
    "LORA_DROPOUT = 0.05                         # dropout       （丢弃率）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9566f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    # 过滤掉不适合的模块\n",
    "    exclude = {'lm_head'}\n",
    "    lora_module_names = [name for name in lora_module_names if name not in exclude]\n",
    "    return lora_module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bca5c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(json_path: str):\n",
    "    \"\"\"从json文件中加载数据\"\"\"\n",
    "    with open(json_path, \"r\") as f:\n",
    "        conversations = json.load(f)\n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80bd887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_examples_from_conversations_single(conversations_list:list[dict]) -> list[dict]:\n",
    "    \"\"\"从conversations中构建单轮对话训练样本\"\"\"\n",
    "    examples = []\n",
    "    for conversations in conversations_list:\n",
    "        idx = 0\n",
    "        system_content = \"\"\n",
    "        conv = conversations['conversations']\n",
    "        if conv[0]['role'] == 'system':\n",
    "            system_content = conv[0]['content']\n",
    "            idx = 1\n",
    "            while idx + 1 < len(conv):\n",
    "                if (conv[idx]['role'] == 'user' and\n",
    "                    conv[idx + 1]['role'] == 'assistant'):\n",
    "                    user_content = conv[idx]['content']\n",
    "                    assistant_content = conv[idx + 1]['content']\n",
    "                    prompt_parts = []\n",
    "                    prompt_parts.append(f\"<|system|>:\\n{system_content}\\n\")\n",
    "                    prompt_parts.append(f\"<|user|>:\\n{user_content}\\n<|assistant|>:\\n\")\n",
    "                    prompt = ''.join(prompt_parts)\n",
    "                    examples.append({\n",
    "                        \"input_text\": prompt,\n",
    "                        \"target_text\": assistant_content\n",
    "                    })\n",
    "                idx += 2\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de41b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_examples_from_conversations_multiple(conversations:list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    从conversations中构建多轮对话训练样本\n",
    "    每个样本包含输入input_text和期望回复target_text\n",
    "    input_text为前面所有对话拼接,\n",
    "    target_text为最后一轮回复\n",
    "    \"\"\"\n",
    "    system_content = \"\"\n",
    "    examples = []\n",
    "    history = []\n",
    "    idx = 0\n",
    "    if len(conversations) > 0 and conversations[0][\"role\"] == \"system\":\n",
    "        system_content = conversations[0].get(\"content\", \"\")\n",
    "        idx = 1\n",
    "\n",
    "    while idx < len(conversations):\n",
    "        #获取当前role和content\n",
    "        current = conversations[idx]\n",
    "        role = current.get(\"role\", \"\")\n",
    "        content = current.get(\"content\", \"\")\n",
    "        \n",
    "        if role == \"user\":\n",
    "            # 检查是否有对应回复\n",
    "            if idx + 1 < len(conversations) and conversations[idx + 1][\"role\"] == \"assistant\":\n",
    "                assistant_content = conversations[idx + 1].get(\"content\", \"\")\n",
    "                \n",
    "                # 构建上下文\n",
    "                prompt_parts = []\n",
    "                if system_content:\n",
    "                    prompt_parts.append(f\"<|system|>:\\n{system_content}\\n\")\n",
    "                \n",
    "                # 添加历史对话\n",
    "                for user_msg, assistant_msg in history:\n",
    "                    prompt_parts.append(f\"<|user|>:\\n{user_msg}\\n<|assistant|>:\\n{assistant_msg}\\n\")\n",
    "                \n",
    "                # 添加当前用户消息\n",
    "                prompt_parts.append(f\"<|user|>:\\n{content}\\n<|assistant|>:\\n\")\n",
    "                prompt = \"\".join(prompt_parts)\n",
    "                \n",
    "                # 添加当前回复\n",
    "                examples.append({\n",
    "                    \"input_text\": prompt,\n",
    "                    \"target_text\": assistant_content\n",
    "                })\n",
    "\n",
    "                # 添加到历史\n",
    "                history.append((content, assistant_content))\n",
    "                idx += 2\n",
    "            # 没有对应的 assistant，跳过这个 user\n",
    "            else:\n",
    "                idx += 1\n",
    "        # 不应该出现单独的 assistant，如果有则跳过\n",
    "        elif role == \"assistant\":\n",
    "            idx += 1\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3129f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(conversations, single_turn=False):\n",
    "    \"\"\"\n",
    "    读取对话数据，构造训练样本\n",
    "    \n",
    "    Args:\n",
    "        conversations: 对话数据\n",
    "        single_turn: 是否提取单轮对话，False为多轮对话\n",
    "    \"\"\"\n",
    "    examples_all = []\n",
    "    if isinstance(conversations, dict) and \"conversations\" in conversations:\n",
    "        # 单个对话对象\n",
    "        conv_list = conversations[\"conversations\"]\n",
    "        if single_turn:\n",
    "            examples = build_examples_from_conversations_single(conv_list)\n",
    "        else:\n",
    "            examples = build_examples_from_conversations_multiple(conv_list)\n",
    "        examples_all.extend(examples)\n",
    "        \n",
    "    elif isinstance(conversations, list):\n",
    "        # 对话列表\n",
    "        if single_turn:\n",
    "            examples_all = build_examples_from_conversations_single(conversations)\n",
    "        else:\n",
    "            for conv in conversations:\n",
    "                if isinstance(conv, dict) and \"conversations\" in conv:\n",
    "                    conv_list = conv[\"conversations\"]\n",
    "                    examples = build_examples_from_conversations_multiple(conv_list)\n",
    "                    examples_all.extend(examples)\n",
    "    return examples_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8bd0a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集加载完成\n",
      "训练样本构造完成\n",
      "构造了 1432 个训练样本示例\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 17\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m构造了 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(examples_all)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m 个训练样本示例\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 加载模型和分词器\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpadding_side \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def main():\n",
    "    \"\"\"主函数，执行微调流程\"\"\"\n",
    "    examples_all = []\n",
    "    # 加载对话数据\n",
    "    conversations = load_data(YAOLAO_JSON)\n",
    "    print(\"数据集加载完成\")\n",
    "    \n",
    "    # 构造训练样本\n",
    "    examples_multiple = prepare_data(conversations)\n",
    "    examples_single = prepare_data(conversations, single_turn=True)\n",
    "    examples_all = examples_multiple + examples_single[:len(examples_multiple)//2]\n",
    "    print(\"训练样本构造完成\")\n",
    "    print(f\"构造了 {len(examples_all)} 个训练样本示例\")\n",
    "    \n",
    "    # 加载模型和分词器\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"分词器加载完成\")\n",
    "    \n",
    "    # 模型加载\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,                     # 使用自定义代码\n",
    "        device_map=\"auto\",                          # 自动分配设备\n",
    "        load_in_8bit=True,                          # 8-bit 模型\n",
    "        # llm_int8_enable_fp32_cpu_offload=True\n",
    "        torch_dtype=torch.float16                   # 使用半精度浮点数\n",
    "    )\n",
    "    print(\"模型加载完成\")\n",
    "\n",
    "    # 查找所有线性层名称\n",
    "    TARGET_MODULES = find_all_linear_names(model)\n",
    "    print(f\"找到的线性层模块: {TARGET_MODULES}\")\n",
    "    \n",
    "    # 使模型为 k-bit/8bit 微调做好准备\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # 配置 LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=TARGET_MODULES,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # 打印可训练参数数量\n",
    "    trainable_params = 0\n",
    "    total_params = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        num = p.numel()\n",
    "        total_params += num\n",
    "        if p.requires_grad:\n",
    "            trainable_params += num\n",
    "    print(f\"可训练参数: {trainable_params} / 总参数: {total_params}\")\n",
    "    \n",
    "    # 构造 Dataset（tokenize 并且 labels 仅包含 assistant）\n",
    "    def tokenize_and_build_labels(example):\n",
    "        \"\"\"\n",
    "        把 input_text 和 target_text 合并： input_ids = tokenize(input_text + target_text)\n",
    "        构造labels,输入部分用-100填充\n",
    "        截断到最大长度 MAX_LENGTH\n",
    "        \"\"\"\n",
    "        input_text = example[\"input_text\"]\n",
    "        target_text = example[\"target_text\"]\n",
    "        full_text = input_text + target_text\n",
    "        \n",
    "        # 编码\n",
    "        input_encoding = tokenizer(input_text, add_special_tokens=False)\n",
    "        target_encoding = tokenizer(target_text, add_special_tokens=False)\n",
    "        \n",
    "        # 合并并截断\n",
    "        input_ids = input_encoding.input_ids + target_encoding.input_ids\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        \n",
    "        # 构造labels\n",
    "        labels = [-100] * len(input_encoding.input_ids) + target_encoding.input_ids\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": [1] * len(input_ids)\n",
    "        }\n",
    "    \n",
    "    # 创建数据集\n",
    "    ds = Dataset.from_list(examples_all)\n",
    "    ds_tokenized = ds.map(\n",
    "        lambda x: tokenize_and_build_labels(x), \n",
    "        remove_columns=ds.column_names, \n",
    "        num_proc=4\n",
    "    )\n",
    "\n",
    "    # 使用Data collator 进行填充\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer, \n",
    "        model=model, \n",
    "        padding=True,\n",
    "        pad_to_multiple_of=8  # 优化GPU效率\n",
    "    )\n",
    "\n",
    "    # 训练参数配置\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,                               # 输出目录\n",
    "        # 批次大小相关\n",
    "        per_device_train_batch_size=PER_DEVICE_BATCH_SIZE,   # 每个GPU的批次大小\n",
    "        gradient_accumulation_steps=GRAD_ACCUM_STEPS,        # 梯度累积步数\n",
    "        # 学习率\n",
    "        warmup_ratio=0.03,                                   # 学习率warmup比例（前3%的step进行warmup）\n",
    "        learning_rate=LEARNING_RATE,                         # 初始学习率\n",
    "        # 训练周期\n",
    "        num_train_epochs=NUM_EPOCHS,                         # 训练的总轮数\n",
    "        # 混合精度训练\n",
    "        fp16=True,                                           # 使用FP16混合精度训练，减少显存使用，加快训练速度\n",
    "        # 日志与保存策略\n",
    "        logging_steps=LOGGING_STEPS,                         # 每多少步记录一次日志\n",
    "        save_strategy=\"steps\",                               # 保存策略 ： 按步数保存模型\n",
    "        save_steps=SAVE_STEPS,                               # 每多少步保存一次模型\n",
    "        save_total_limit=3,                                  # 最多保存模型数量\n",
    "        # 数据列处理\n",
    "        remove_unused_columns=False,                         # 不要移除未使用的列\n",
    "        report_to=\"none\",                                    # 不使用任何报告工具（如 WandB）\n",
    "        # 优化器配置\n",
    "        optim=\"paged_adamw_8bit\",                            # 使用 8-bit Adam 优化器\n",
    "    )\n",
    "\n",
    "    # 训练器\n",
    "    trainer = Trainer(\n",
    "        model=model,                    \n",
    "        args=training_args,\n",
    "        train_dataset=ds_tokenized,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # 开始训练\n",
    "    trainer.train()\n",
    "\n",
    "    # 保存 LoRA 权重\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"训练完成并将 LoRA 权重保存到 {OUTPUT_DIR}\")\n",
    "    \n",
    "    # 清理内存\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yaolao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
