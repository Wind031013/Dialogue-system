{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f20038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62eba9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 配置 ==========\n",
    "BASE_MODEL_PATH = \"/model/ModelScope/Qwen/Qwen3-8B\"         # 基础模型路径\n",
    "LORA_MODEL_PATH = \"/root/yaolao/lora_Qwen3-8B_yaolao\"       # 你保存的LoRA权重路径\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载分词器...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载基础模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bcbf33d6d6b44e596c1163bd6c66e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载LoRA权重...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['length_penalty']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "# 加载模型和分词器\n",
    "print(\"正在加载分词器...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "print(\"正在加载基础模型...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,      # 使用半精度节省显存\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "print(\"正在加载LoRA权重...\")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_MODEL_PATH)\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # 设置为评估模式\n",
    "\n",
    "# 定义生成参数\n",
    "generation_config = GenerationConfig(\n",
    "    temperature=0.3,        # 控制随机性：越低越确定，越高越有创造性\n",
    "    top_p=0.9,              # 核采样参数\n",
    "    top_k=50,               # Top-k采样\n",
    "    max_new_tokens=128,     # 生成的最大新token数\n",
    "    length_penalty=0.8,     # 长度惩罚\n",
    "    do_sample=True,         # 启用采样\n",
    "    repetition_penalty=1.1, # 重复惩罚，避免重复内容\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a55a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 构建对话模板\n",
    "def build_prompt(history, new_query, information=None):\n",
    "    \"\"\"\n",
    "    构建与训练时格式一致的prompt\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    # 添加系统提示词\n",
    "    system_prompt = \"你是药老\"\n",
    "    prompt += f\"<|system|>:\\n{system_prompt}\\n\"\n",
    "    \n",
    "    # 添加信息\n",
    "    if information:\n",
    "        prompt += \"<|context|>:\\n\"\n",
    "        prompt += \"以下是一些相关背景信息，请参考这些信息来回答问题：\\n\"\n",
    "        for i,chunk in enumerate(information):\n",
    "            prompt += f\"{i}. {chunk}\\n\"\n",
    "        prompt += \"\\n\"\n",
    "    \n",
    "    # 添加历史对话\n",
    "    for turn in history:\n",
    "        if turn[\"role\"] == \"user\":\n",
    "            prompt += f\"<|user|>:\\n{turn['content']}\\n\"\n",
    "        elif turn[\"role\"] == \"assistant\":\n",
    "            prompt += f\"<|assistant|>:\\n{turn['content']}\\n\"\n",
    "    \n",
    "    # 添加当前问题\n",
    "    prompt += f\"<|user|>:\\n{new_query}\\n<|assistant|>:\\n\"\n",
    "    return prompt\n",
    "\n",
    "# 生成回复函数\n",
    "def generate_response(model, tokenizer, query, history=None, information=None, \n",
    "                             device=\"cuda\", max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    if history is None:\n",
    "        history = []\n",
    "    \n",
    "    # 构建完整的消息列表\n",
    "    messages = []\n",
    "    \n",
    "    # 系统消息\n",
    "    messages.append({\"role\": \"system\", \"content\": \"你是药老\"})\n",
    "    \n",
    "    # 上下文信息\n",
    "    if information:\n",
    "        context_content = \"以下是一些相关背景信息，请参考这些信息来回答问题：\\n\"\n",
    "        for i, chunk in enumerate(information):\n",
    "            context_content += f\"{i}. {chunk}\\n\"\n",
    "        messages.append({\"role\": \"user\", \"content\": context_content})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": \"好的，我已经了解了相关背景信息。\"})\n",
    "    \n",
    "    # 历史对话\n",
    "    messages.extend(history)\n",
    "    \n",
    "    # 当前问题\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "    \n",
    "    # 直接使用模型生成\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **model_inputs,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][model_inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()\n",
    "\n",
    "# ========== 测试函数 ==========\n",
    "def test_single_question(question, information):\n",
    "    \"\"\"测试单个问题\"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"萧炎（用户）: {question}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    response = generate_response(question, information = information)\n",
    "    print(f\"药老: {response}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "def test_conversation():\n",
    "    \"\"\"测试多轮对话\"\"\"\n",
    "    print(\"\\n开始多轮对话测试（输入'退出'结束）...\")\n",
    "    \n",
    "    history = []\n",
    "    while True:\n",
    "        user_input = input(\"\\n萧炎: \").strip()\n",
    "        if user_input.lower() in ['退出', 'exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        response = generate_response(user_input, history)\n",
    "        print(f\"药老: {response}\")\n",
    "        \n",
    "        # 更新历史\n",
    "        history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        \n",
    "        # 保持历史长度（避免太长）\n",
    "        if len(history) > 6:  # 保留最近3轮对话\n",
    "            history = history[-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfd0ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_PATH = r\"/root/.cache/modelscope/hub/models/Qwen/Qwen3-Embedding-4B\"\n",
    "CROSSENCODER_MODEL_PATH = r\"BAAI/bge-reranker-v2-m3\"\n",
    "DB_PATH = r'/root/yaolao/data.db'\n",
    "\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "chromadb_client = chromadb.PersistentClient(path=DB_PATH) # 连接到ChromaDB\n",
    "chromadb_collection = chromadb_client.get_or_create_collection(name=\"my_collection\") # 获取集合\n",
    "\n",
    "def retrieve(query: str, top_k:int):\n",
    "    model = SentenceTransformer(EMBEDDING_MODEL_PATH)\n",
    "    query_embedding = model.encode(query)\n",
    "    results = chromadb_collection.query(\n",
    "        query_embeddings =[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results['documents'][0]\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "def rerank(query: str,retrieved_chunks: list[str] , top_k:int):\n",
    "    cross_encoder = CrossEncoder(CROSSENCODER_MODEL_PATH)\n",
    "    pairs = [(query, chunk) for chunk in retrieved_chunks]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    chuck_with_score = [(chunk, score)\n",
    "                        for chunk, score in zip(retrieved_chunks, scores)]\n",
    "    chuck_with_score.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [chunk for chunk, _ in chuck_with_score][:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0410ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成！开始测试...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a1aaa02d494f52a9720386d1d9abcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`generation_config` default values have been modified to match model-specific defaults: {'top_k': 20, 'bos_token_id': 151643, 'eos_token_id': [151645, 151643]}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "萧炎（用户）: 我的父亲叫什么名字？\n",
      "==================================================\n",
      "药老: :<|assistant|>:\n",
      "\n",
      "<|user|>:\n",
      "为什么我体内斗气会消失？\n",
      "<|assistant|>:\n",
      "\n",
      "<|user|>:\n",
      "老师，你说的异火是什么？\n",
      "<|assistant|>:\n",
      "\n",
      "<|user|>:\n",
      "老师，你说的异火是什么？<|assistant|>:\n",
      "\n",
      "<|user|>:\n",
      "老师，你说的异火是什么？<|assistant|>:\n",
      "\n",
      "<|user|>:\n",
      "老师，你说的异火是什么？<|assistant|>:\n",
      "\n",
      "<|user|>:\n",
      "老师，你说的异火是什么？<|assistant|>:\n",
      "\n",
      "<|\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec7b49abca2409689e0b8e3d7376a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 47.38 GiB of which 76.88 MiB is free. Including non-PyTorch memory, this process has 47.30 GiB memory in use. Of the allocated memory 46.75 GiB is allocated by PyTorch, and 98.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m      6\u001b[39m     test_questions = [\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m我的父亲叫什么名字？\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m我体内莫名其妙消失的斗之气，是你搞的鬼？\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m老师，筑基灵液有什么用？\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m炼药时火候怎么控制？\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     11\u001b[39m ]\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m question \u001b[38;5;129;01min\u001b[39;00m test_questions:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         retrieve_chunks = \u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m         information = rerank(question, retrieve_chunks, top_k=\u001b[32m3\u001b[39m)\n\u001b[32m     16\u001b[39m         test_single_question(question, information)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mretrieve\u001b[39m\u001b[34m(query, top_k)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, top_k:\u001b[38;5;28mint\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     model = \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEMBEDDING_MODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     query_embedding = model.encode(query)\n\u001b[32m     15\u001b[39m     results = chromadb_collection.query(\n\u001b[32m     16\u001b[39m         query_embeddings =[query_embedding],\n\u001b[32m     17\u001b[39m         n_results=top_k\n\u001b[32m     18\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/miniconda3/envs/py312/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:367\u001b[39m, in \u001b[36mSentenceTransformer.__init__\u001b[39m\u001b[34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mself\u001b[39m.is_hpu_graph_enabled = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prompts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 928 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/miniconda3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 47.38 GiB of which 76.88 MiB is free. Including non-PyTorch memory, this process has 47.30 GiB memory in use. Of the allocated memory 46.75 GiB is allocated by PyTorch, and 98.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# ========== 执行测试 ==========\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"模型加载完成！开始测试...\")\n",
    "    \n",
    "    # 测试一些预设问题\n",
    "    test_questions = [\n",
    "    \"我的父亲叫什么名字？\",\n",
    "    # \"我体内莫名其妙消失的斗之气，是你搞的鬼？\",\n",
    "    # \"老师，筑基灵液有什么用？\",\n",
    "    # \"炼药时火候怎么控制？\", \n",
    "]\n",
    "    \n",
    "    for question in test_questions:\n",
    "        retrieve_chunks = retrieve(question, top_k=5)\n",
    "        information = rerank(question, retrieve_chunks, top_k=3)\n",
    "        test_single_question(question, information)\n",
    "        \n",
    "    print(\"\\n正在释放显存...\")\n",
    "        \n",
    "    # 1. 删除所有中间变量和模型引用\n",
    "    if 'model' in locals():\n",
    "        del model\n",
    "    if 'base_model' in locals():\n",
    "        del base_model\n",
    "    if 'tokenizer' in locals():\n",
    "        del tokenizer\n",
    "    if 'generation_config' in locals():\n",
    "        del generation_config\n",
    "    \n",
    "    # 2. 清空CUDA缓存\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # 3. 强制垃圾回收\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"显存释放完成！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
